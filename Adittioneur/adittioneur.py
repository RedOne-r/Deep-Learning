# -*- coding: utf-8 -*-
"""Adittioneur.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mTpmwVDVC416U-8UBya_bquldTTrPK7j
"""

import torch.nn as nn
import torch.optim
import torch.optim as optim
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader


class Encoder(nn.Module):
    def __init__(self, input_size_enc, hidden_size_enc, num_layers=1):
        super().__init__()
        self.rnn = nn.GRU(input_size_enc, hidden_size_enc, num_layers, batch_first=True)

    def forward(self, x_enc, h0=None):
        out, hn = self.rnn(x_enc, h0)
        return hn



class Decoder(nn.Module):
    def __init__(self, input_size_dec, hidden_size_dec, output_size, num_layers=1):
        super().__init__()
        self.rnn = nn.GRU(input_size_dec, hidden_size_dec, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size_dec, output_size)

    def forward(self, y, h0):
        out, hn = self.rnn(y, h0)
        logits = self.fc(out)
        return logits, hn


class Seq2seq(nn.Module):
    def __init__(self, encoder:Encoder , decoder:Decoder ):
        super().__init__()
        self.encoder=encoder
        self.decoder=decoder

    def forward(self,x,y, h0=None):
        outpout_enc=self.encoder(x,h0)
        logits,hn = self.decoder(y,outpout_enc)
        return logits, hn

with open("datasetF.pkl", "rb") as f:
    data = pickle.load(f)

vocab = {
    "0": 0,
    "1": 1,
    "2": 2,
    "3": 3,
    "4": 4,
    "5": 5,
    "6": 6,
    "7": 7,
    "8": 8,
    "9": 9,
    "+": 10,
"<sos>": 11,
"<eos>": 12
}

'''
def OneHot(vocab, adittion):
    data_opinion = []
    max_len_input= 9
    max_len_target = 7

    for sentence, label in adittion:
        S = []
        sentence = sentence.lower()
        digits = list(sentence)

        for d in digits:
            vector = [0] * len(vocab)
            vector[vocab[d]] = 1
            S.append(vector)

        if len(S) < max_len_input:
            pad_vector = [0] * len(vocab)
            S += [pad_vector] * (max_len_input - len(S))
        else:
            S = S[:max_len_input]

        Y = []
        vector = [0] * len(vocab)
        vector[vocab["<sos>"]] = 1
        Y.append(vector)

        for d in list(str(label)):
            vec = [0] * len(vocab)
            vec[vocab[d]] = 1
            Y.append(vec)
        eos_vec = [0] * len(vocab)
        eos_vec[vocab["<eos>"]] = 1
        Y.append(eos_vec)

        if len(Y) < max_len_target:
            pad_vec = [0] * len(vocab)
            Y += [pad_vec] * (max_len_target - len(Y))
        else:
            Y = Y[:max_len_target]

        data_opinion.append((S, Y))

    return np.array(data_opinion, dtype=object)


def OneHot(vocab, adittion):
    data_opinion = []
    max_len_input= 9
    max_len_target = 7

    for sentence, label in adittion:
        S = []
        sentence = sentence.lower()
        digits = list(sentence)
        plus_idx = digits.index("+") if "+" in digits else -1  # AJOUTÉ

        for i, d in enumerate(digits):  # MODIFIÉ
            vector = [0] * len(vocab)
            if d.isdigit():  # AJOUTÉ
                if plus_idx != -1:  # AJOUTÉ
                    if i < plus_idx:  # AJOUTÉ
                        rank = (plus_idx - 1) - i  # AJOUTÉ
                    elif i > plus_idx:  # AJOUTÉ
                        rank = (len(digits) - 1) - i  # AJOUTÉ
                    else:  # AJOUTÉ
                        rank = 0  # AJOUTÉ
                else:  # AJOUTÉ
                    rank = (len(digits) - 1) - i  # AJOUTÉ
                vector[vocab[d]] = 1 + rank  # MODIFIÉ
            else:  # AJOUTÉ
                vector[vocab[d]] = 1  # AJOUTÉ
            S.append(vector)

        if len(S) < max_len_input:
            pad_vector = [0] * len(vocab)
            S += [pad_vector] * (max_len_input - len(S))
        else:
            S = S[:max_len_input]

        Y = []
        vector = [0] * len(vocab)
        vector[vocab["<sos>"]] = 1
        Y.append(vector)

        for d in list(str(label)):
            vec = [0] * len(vocab)
            vec[vocab[d]] = 1
            Y.append(vec)
        eos_vec = [0] * len(vocab)
        eos_vec[vocab["<eos>"]] = 1
        Y.append(eos_vec)

        if len(Y) < max_len_target:
            pad_vec = [0] * len(vocab)
            Y += [pad_vec] * (max_len_target - len(Y))
        else:
            Y = Y[:max_len_target]

        data_opinion.append((S, Y))

    return np.array(data_opinion, dtype=object)
'''
def OneHot(vocab, adittion):
    data_opinion = []
    max_len_input = 9
    max_len_target = 7

    for sentence, label in adittion:
        # ----------------------
        # 1) ENTRÉE (S) : même logique qu’avant
        # ----------------------
        S = []
        sentence = sentence.lower()
        digits = list(sentence)
        plus_idx = digits.index("+") if "+" in digits else -1

        for i, d in enumerate(digits):
            vector = [0] * len(vocab)
            if d.isdigit():
                if plus_idx != -1:
                    if i < plus_idx:
                        rank = (plus_idx - 1) - i
                    elif i > plus_idx:
                        rank = (len(digits) - 1) - i
                    else:
                        rank = 0
                else:
                    rank = (len(digits) - 1) - i
                vector[vocab[d]] = 1 + rank
            else:
                vector[vocab[d]] = 1
            S.append(vector)

        # padding / tronquage entrée
        if len(S) < max_len_input:
            pad_vector = [0] * len(vocab)
            S += [pad_vector] * (max_len_input - len(S))
        else:
            S = S[:max_len_input]

        # ----------------------
        # 2) SORTIE (Y) : chiffres INVERSES (unités d’abord)
        # ----------------------
        Y = []
        # <sos>
        vec_sos = [0] * len(vocab)
        vec_sos[vocab["<sos>"]] = 1
        Y.append(vec_sos)

        # label en chaînes, puis inversion des chiffres
        label_str = str(label)
        label_digits = list(label_str)[::-1]  # <-- inversion ici (unités en premier)

        for i, d in enumerate(label_digits):
            vec = [0] * len(vocab)
            # rang réel : unités = 0, dizaines = 1, etc.
            rank_out = i
            vec[vocab[d]] = 1 + rank_out
            Y.append(vec)

        # <eos>
        eos_vec = [0] * len(vocab)
        eos_vec[vocab["<eos>"]] = 1
        Y.append(eos_vec)

        # padding / tronquage sortie
        if len(Y) < max_len_target:
            pad_vec = [0] * len(vocab)
            Y += [pad_vec] * (max_len_target - len(Y))
        else:
            Y = Y[:max_len_target]

        data_opinion.append((S, Y))

    return np.array(data_opinion, dtype=object)

data_opinion = OneHot(vocab, data)


class IMDBDataset(Dataset):
    def __init__(self, data):
        self.data = data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        X, y = self.data[idx]
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.float32)
        return X, y


dataset = IMDBDataset(data_opinion)


from torch.utils.data import random_split

train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

loader = DataLoader(train_dataset, batch_size=32, shuffle=True)   # remplace le loader par le train_loader
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)


input_size_enc = 13
input_size_dec = 13

hidden_size_enc = 128
hidden_size_dec = 128

output_size = 13
num_layers = 1
num_epochs = 40
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

encoder = Encoder(input_size_enc, hidden_size_enc, num_layers)
decoder = Decoder(input_size_dec, hidden_size_dec, output_size, num_layers)

model = Seq2seq(encoder, decoder).to(device)

pad_idx = -100  # AJOUTÉ
criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)  # MODIFIÉ
optimizer = optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    total_loss = 0
    correct = 0
    total = 0
    for X, y in loader:
        X, y = X.to(device), y.to(device)

        y_in = y[:, :-1, :]
        y_out_idx = y[:, 1:, :].argmax(dim=2).long()

        outputs, _ = model(X, y_in)

        B, T, V = outputs.shape
        y_out_idx_flat = y_out_idx.reshape(B*T)  # AJOUTÉ
        outputs_flat = outputs.reshape(B*T, V)   # AJOUTÉ

        # on marque les positions de padding comme ignore_index
        mask_pad = (y[:, 1:, :].sum(dim=2) == 0).reshape(B*T)  # AJOUTÉ
        y_out_idx_flat[mask_pad] = pad_idx  # AJOUTÉ

        loss = criterion(outputs_flat, y_out_idx_flat)  # MODIFIÉ

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        pred_idx = outputs.argmax(dim=2)
        mask_pad_2 = (y[:, 1:, :].sum(dim=2) != 0)  # AJOUTÉ
        correct += ((pred_idx == y_out_idx) & mask_pad_2).sum().item()  # AJOUTÉ
        total += mask_pad_2.sum().item()  # AJOUTÉ

    acc = 100 * correct / total if total > 0 else 0
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%")

"""**TEST**"""

# --- Évaluation finale sur le jeu de test ---
model.eval()
test_loss, test_correct, test_total = 0, 0, 0
with torch.no_grad():
    for X, y in test_loader:
        X, y = X.to(device), y.to(device)
        y_in = y[:, :-1, :]
        y_out_idx = y[:, 1:, :].argmax(dim=2).long()
        outputs, _ = model(X, y_in)
        B, T, V = outputs.shape
        y_out_idx_flat = y_out_idx.reshape(B*T)
        outputs_flat = outputs.reshape(B*T, V)
        mask_pad = (y[:, 1:, :].sum(dim=2) == 0).reshape(B*T)
        y_out_idx_flat[mask_pad] = pad_idx
        loss = criterion(outputs_flat, y_out_idx_flat)
        test_loss += loss.item()
        pred_idx = outputs.argmax(dim=2)
        mask_pad_2 = (y[:, 1:, :].sum(dim=2) != 0)
        test_correct += ((pred_idx == y_out_idx) & mask_pad_2).sum().item()
        test_total += mask_pad_2.sum().item()

test_acc = 100 * test_correct / test_total if test_total > 0 else 0
print(f"\n>>> Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%")

# dictionnaire inverse
idx_to_token = {idx: tok for tok, idx in vocab.items()}

def predict_add(num1: str, num2: str):
    """
    Utilise le modèle entraîné pour prédire num1 + num2.
    num1 et num2 doivent être des chaînes (ex: "123", "56")
    Renvoie une chaîne correspondant au résultat (ex: "179").
    """

    expr = num1 + "+" + num2

    # ----------------------
    # encodage de l’expression d’entrée (comme dans OneHot)
    # ----------------------
    digits = list(expr)
    plus_idx = digits.index("+") if "+" in digits else -1

    S = []
    for i, d in enumerate(digits):
        vec = [0] * len(vocab)
        if d.isdigit():
            if plus_idx != -1:
                if i < plus_idx:
                    rank = (plus_idx - 1) - i
                elif i > plus_idx:
                    rank = (len(digits) - 1) - i
                else:
                    rank = 0
            else:
                rank = (len(digits) - 1) - i
            vec[vocab[d]] = 1 + rank
        else:
            vec[vocab[d]] = 1
        S.append(vec)

    # padding à 9
    if len(S) < 9:
        pad = [0] * len(vocab)
        S += [pad] * (9 - len(S))
    else:
        S = S[:9]

    # conversion en tenseur (batch=1)
    X = torch.tensor([S], dtype=torch.float32).to(device)

    # ----------------------
    # DÉCODAGE pas-à-pas
    # ----------------------
    model.eval()
    with torch.no_grad():
        # encode
        h = model.encoder(X)

        # premier token = <sos>
        V = len(vocab)
        y_t = torch.zeros(1, 1, V, device=device)
        y_t[0, 0, vocab["<sos>"]] = 1

        generated = []

        for _ in range(6):  # max target - 1 (sans <sos>)
            out, h = model.decoder(y_t, h)
            logits = out[:, -1, :]
            idx = torch.argmax(logits, dim=1).item()
            tok = idx_to_token[idx]

            if tok == "<eos>":
                break

            generated.append(tok)

            # préparer le prochain y_t
            y_next = torch.zeros(1, 1, V, device=device)
            y_next[0, 0, idx] = 1
            y_t = y_next

    # ⚠️ generated est à l’envers (unités, dizaines,...) → on inverse
    generated = generated[::-1]

    # on obtient le nombre final
    return "".join(generated)

predict_add("587","789")